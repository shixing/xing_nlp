{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 预处理数据\n",
    "\n",
    "### 1.1 创建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(r\"([.,!?\\\"':;)(])\") # 将这些作为分隔符\n",
    "_DIGIT_RE = re.compile(r\"\\d\")\n",
    "\n",
    "def blank_tokenizer(sentence):\n",
    "    return sentence.strip().split()\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(vocabulary_path, data_paths, max_vocabulary_size,\n",
    "                                            tokenizer=None, normalize_digits=False):\n",
    "    if not gfile.Exists(vocabulary_path):\n",
    "        print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, \",\".join(data_paths)))\n",
    "        vocab = {}\n",
    "        for data_path in data_paths:\n",
    "            with gfile.GFile(data_path, mode=\"r\") as f:\n",
    "                print(data_path)\n",
    "                counter = 0\n",
    "                for line in f:\n",
    "                    counter += 1\n",
    "                    if counter % 100000 == 0:\n",
    "                        print(\"  processing line %d\" % counter)\n",
    "#                     line = tf.compat.as_bytes(line)\n",
    "                    tokens = tokenizer(line) if tokenizer else blank_tokenizer(line)\n",
    "                    for w in tokens:\n",
    "                        word = _DIGIT_RE.sub(r\"0\", w) if normalize_digits else w\n",
    "                        if word in vocab:\n",
    "                            vocab[word] += 1\n",
    "                        else:\n",
    "                            vocab[word] = 1\n",
    "                print(len(vocab))\n",
    "        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        if len(vocab_list) > max_vocabulary_size:\n",
    "            vocab_list = vocab_list[:max_vocabulary_size]\n",
    "        with gfile.GFile(vocabulary_path, mode=\"w\") as vocab_file:\n",
    "            for w in vocab_list:\n",
    "#                 print(w)\n",
    "                vocab_file.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = \"./model/model_small\"\n",
    "train_path = \"./data/small/train\"\n",
    "dev_path = \"./data/small/valid\"\n",
    "test_path = \"./data/small/test\"\n",
    "vocab_size = 100\n",
    "L = 15\n",
    "n_bucket = 3\n",
    "data_cache_dir = os.path.join(model_dir,\"data_cache\")\n",
    "saved_model_dir = os.path.join(model_dir,\"saved_model\")\n",
    "vocab_path = os.path.join(data_cache_dir, \"vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary ./model/model_small/data_cache/vocab from data ./data/small/train,./data/small/valid\n",
      "./data/small/train\n",
      "26\n",
      "./data/small/valid\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "create_vocabulary(vocab_path, [train_path, dev_path], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_vocabulary(vocabulary_path):\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip() for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary,\n",
    "                                                    tokenizer=None, normalize_digits=False, with_start = True, with_end = True):\n",
    "    \n",
    "    if tokenizer:\n",
    "        words = tokenizer(sentence)\n",
    "    else:\n",
    "        words = basic_tokenizer(sentence)\n",
    "    if not normalize_digits:\n",
    "        ids =  [vocabulary.get(w, UNK_ID) for w in words]\n",
    "    # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "    else:\n",
    "        ids =  [vocabulary.get(_DIGIT_RE.sub(r\"0\", w), UNK_ID) for w in words]\n",
    "    if with_start:\n",
    "            ids = [GO_ID] + ids\n",
    "    if with_end:\n",
    "            ids =  ids + [EOS_ID]\n",
    "    return ids        \n",
    "        \n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                                            tokenizer=None, normalize_digits=False, with_go = True, with_end = True):\n",
    "    if not gfile.Exists(target_path):\n",
    "        print(\"Tokenizing data in %s\" % data_path)\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"r\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 100000 == 0:\n",
    "                        print(\"  tokenizing line %d\" % counter)\n",
    "                    token_ids = sentence_to_token_ids(line, vocab,tokenizer, normalize_digits)\n",
    "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in ./data/small/train\n"
     ]
    }
   ],
   "source": [
    "train_ids_path =  os.path.join(data_cache_dir, \"train.ids\")\n",
    "data_to_token_ids(train_path, train_ids_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in ./data/small/valid\n"
     ]
    }
   ],
   "source": [
    "# Create token ids for the development data.\n",
    "dev_ids_path = os.path.join(data_cache_dir, \"dev.ids\")\n",
    "data_to_token_ids(dev_path, dev_ids_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(cache_dir, train_path, dev_path, vocabulary_size):\n",
    "    \"\"\"Preapre all necessary files that are required for the training.\n",
    "\n",
    "        Args:\n",
    "            data_dir: directory in which the data sets will be stored.\n",
    "            all the sentence already prepend _GO and append _EOS\n",
    "\n",
    "    \"\"\"\n",
    "    # Create vocabularies of the appropriate sizes.\n",
    "    vocab_path = os.path.join(cache_dir, \"vocab\")\n",
    "    create_vocabulary(vocab_path, [train_path, dev_path], vocabulary_size)\n",
    "\n",
    "    # Create token ids for the training data.\n",
    "    train_ids_path =  os.path.join(cache_dir, \"train.ids\")\n",
    "    data_to_token_ids(train_path, train_ids_path, vocab_path)\n",
    "\n",
    "    # Create token ids for the development data.\n",
    "    dev_ids_path = os.path.join(cache_dir, \"dev.ids\")\n",
    "    data_to_token_ids(dev_path, dev_ids_path, vocab_path)\n",
    "\n",
    "    return train_ids_path, dev_ids_path, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary ./model/model_small/data_cache/vocab from data ./data/small/train,./data/small/valid\n",
      "./data/small/train\n",
      "26\n",
      "./data/small/valid\n",
      "26\n",
      "Tokenizing data in ./data/small/train\n",
      "Tokenizing data in ./data/small/valid\n"
     ]
    }
   ],
   "source": [
    "train_ids_path, dev_ids_path, vocab_path  = prepare_data(data_cache_dir, train_path, dev_path, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_raw_data(target_path, max_size=None):\n",
    "    '''\n",
    "    Args: \n",
    "        target_path : the path which contains word ids\n",
    "    '''\n",
    "    print(\"read raw data from {}\".format(target_path))\n",
    "    data_set = []\n",
    "    data_length = []\n",
    "\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "        target = target_file.readline()\n",
    "        counter = 0\n",
    "        while target and (not max_size or counter < max_size):\n",
    "            counter += 1\n",
    "            if counter % 100000 == 0:\n",
    "                print(\"  reading data line %d\" % counter)\n",
    "                sys.stdout.flush()\n",
    "            target_ids = [int(x) for x in target.split()]\n",
    "            data_set.append(target_ids)\n",
    "            data_length.append(len(target_ids))\n",
    "            target = target_file.readline()\n",
    "\n",
    "\n",
    "    return data_set, data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read raw data from ./model/model_small/data_cache/train.ids\n"
     ]
    }
   ],
   "source": [
    "train_data, train_length = read_raw_data(train_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read raw data from ./model/model_small/data_cache/dev.ids\n"
     ]
    }
   ],
   "source": [
    "dev_data, dev_length = read_raw_data(dev_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_array = train_length + dev_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_buckets(length_array, max_length, max_buckets):\n",
    "    d = {} \n",
    "    for length in length_array:\n",
    "        if not length in d:\n",
    "            d[length] = 0\n",
    "        d[length] += 1\n",
    "    \n",
    "    dd = [(x, d[x]) for x in d]\n",
    "    dd = sorted(dd, key = lambda x: x[0])\n",
    "    running_sum = []\n",
    "    s = 0\n",
    "    for l, n in dd:\n",
    "        s += n\n",
    "        running_sum.append((l,s))\n",
    "    \n",
    "    def best_point(ll):\n",
    "        # return index so that l[:index+1] and l[index+1:]\n",
    "        index = 0\n",
    "        maxv = 0\n",
    "        base = ll[0][1]\n",
    "        for i in range(len(ll)):\n",
    "            l,n = ll[i]\n",
    "            v = (ll[-1][0] - l) * (n-base)\n",
    "            if v > maxv:\n",
    "                maxv = v\n",
    "                index = i\n",
    "        return index, maxv\n",
    "    \n",
    "    def arg_max(array,key):\n",
    "        maxv = -10000\n",
    "        index = -1\n",
    "        for i in range(len(array)):\n",
    "            item = array[i]\n",
    "            v = key(item)\n",
    "            if v > maxv:\n",
    "                maxv = v\n",
    "                index = i\n",
    "        return index\n",
    "\n",
    "    end_index = 0\n",
    "    for i in range(len(running_sum)-1,-1,-1):\n",
    "        if running_sum[i][0] <= max_length:\n",
    "            end_index = i+1\n",
    "            break\n",
    "\n",
    "    print(\"running_sum [(length, count)] :\")\n",
    "    print(running_sum)\n",
    "\n",
    "    if end_index <= max_buckets:\n",
    "        buckets = [x[0] for x in running_sum[:end_index]]\n",
    "    else:\n",
    "        buckets = []\n",
    "        # (array,  maxv, index)\n",
    "        states = [(running_sum[:end_index],0,end_index-1)]\n",
    "        while len(buckets) < max_buckets:\n",
    "            index = arg_max(states, lambda x: x[1])\n",
    "            state = states[index]\n",
    "            del states[index]\n",
    "            #split state\n",
    "            array = state[0]\n",
    "            split_index = state[2]\n",
    "            buckets.append(array[split_index][0])\n",
    "            array1 = array[:split_index+1]\n",
    "            array2 = array[split_index+1:]\n",
    "            if len(array1) > 0:\n",
    "                id1, maxv1 = best_point(array1)\n",
    "                states.append((array1,maxv1,id1))\n",
    "            if len(array2) > 0:\n",
    "                id2, maxv2 = best_point(array2)\n",
    "                states.append((array2,maxv2,id2))\n",
    "    return sorted(buckets)\n",
    "\n",
    "\n",
    "def split_buckets(array,buckets,withOrder = False):\n",
    "    \"\"\"\n",
    "    array : [[items]]\n",
    "    return:\n",
    "    d : [[[items]]]\n",
    "    order: [(bucket_id, index_in_bucket)]\n",
    "    \"\"\"\n",
    "    order = []\n",
    "    d = [[] for i in range(len(buckets))]\n",
    "    for items in array:\n",
    "        index = get_buckets_id(len(items), buckets)\n",
    "        if index >= 0:\n",
    "            d[index].append(items)\n",
    "            order.append((index, len(d[index])-1))\n",
    "    \n",
    "    return d, order\n",
    "\n",
    "\n",
    "\n",
    "def get_buckets_id(l, buckets):\n",
    "    id = -1\n",
    "    for i in range(len(buckets)):\n",
    "        if l <= buckets[i]:\n",
    "            id = i\n",
    "            break\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_sum [(length, count)] :\n",
      "[(3, 55), (4, 109), (5, 158), (6, 224), (7, 278), (8, 325), (9, 382), (10, 449), (11, 496), (12, 547), (13, 591), (14, 647), (15, 716), (16, 772), (17, 823), (18, 871), (19, 917), (20, 986), (21, 1043), (22, 1100)]\n"
     ]
    }
   ],
   "source": [
    "_buckets = calculate_buckets(length_array, L, n_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_bucket,_ = split_buckets(train_data, _buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_bucket,order = split_buckets(train_data, _buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_bucket,_ = split_buckets(dev_data, _buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_dev(cache_dir, train_path, dev_path, vocab_size, max_length, n_bucket):\n",
    "    train_ids_path, dev_ids_path, vocab_path  = prepare_data(cache_dir, train_path, dev_path, vocab_size)\n",
    "    train_data, train_length = read_raw_data(train_ids_path)\n",
    "    dev_data, dev_length = read_raw_data(dev_ids_path)\n",
    "    length_array = train_length + dev_length\n",
    "    _buckets = calculate_buckets(length_array, max_length, n_bucket)\n",
    "    train_data_bucket,_ = split_buckets(train_data, _buckets)\n",
    "    dev_data_bucket,_ = split_buckets(dev_data, _buckets)\n",
    "    return train_data_bucket, dev_data_bucket, _buckets, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read raw data from ./model/model_small/data_cache/train.ids\n",
      "read raw data from ./model/model_small/data_cache/dev.ids\n",
      "running_sum [(length, count)] :\n",
      "[(3, 55), (4, 109), (5, 158), (6, 224), (7, 278), (8, 325), (9, 382), (10, 449), (11, 496), (12, 547), (13, 591), (14, 647), (15, 716), (16, 772), (17, 823), (18, 871), (19, 917), (20, 986), (21, 1043), (22, 1100)]\n"
     ]
    }
   ],
   "source": [
    "train_data_bucket, dev_data_bucket, _buckets, vocab_path = read_train_dev(data_cache_dir, train_path, dev_path, vocab_size, L, n_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 15]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_real_vocab_size(vocab_path):\n",
    "    n = 0\n",
    "    f = open(vocab_path)\n",
    "    for line in f:\n",
    "        n+=1\n",
    "    f.close()\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vocab_size = get_real_vocab_size(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_tokens = np.sum([np.sum([len(items) for items in x]) for x in train_data_bucket])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5940"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[205, 205, 245]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bucket_sizes = [len(train_data_bucket[b]) for b in range(len(_buckets))]\n",
    "train_bucket_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 15]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_total_size = float(sum(train_bucket_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31297709923664124, 0.6259541984732825, 1.0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in range(len(train_bucket_sizes))]\n",
    "train_buckets_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_bucket_sizes = [len(dev_data_bucket[b]) for b in range(len(_buckets))]\n",
    "dev_total_size = int(sum(dev_bucket_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = int(train_total_size / batch_size)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_per_dev = int(dev_total_size / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_per_checkpoint = int(steps_per_epoch / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_steps = steps_per_epoch * n_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mylog(msg):\n",
    "    print(msg)\n",
    "#     sys.stdout.flush()\n",
    "#     logging.info(msg)\n",
    "def mylog_section(section_name):\n",
    "    mylog(\"======== {} ========\".format(section_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_vocab_size: 30\n",
      "_buckets: [6, 10, 15]\n",
      "Train:\n",
      "total: 655.0\n",
      "bucket sizes: [205, 205, 245]\n",
      "Dev:\n",
      "total: 61\n",
      "bucket sizes: [19, 20, 22]\n",
      "Steps_per_epoch: 163\n",
      "Total_steps:16300\n",
      "Steps_per_checkpoint: 81\n"
     ]
    }
   ],
   "source": [
    "# reports\n",
    "mylog(\"real_vocab_size: {}\".format(real_vocab_size))\n",
    "mylog(\"_buckets: {}\".format(_buckets))\n",
    "mylog(\"Train:\")\n",
    "mylog(\"total: {}\".format(train_total_size))\n",
    "mylog(\"bucket sizes: {}\".format(train_bucket_sizes))\n",
    "mylog(\"Dev:\")\n",
    "mylog(\"total: {}\".format(dev_total_size))\n",
    "mylog(\"bucket sizes: {}\".format(dev_bucket_sizes))\n",
    "mylog(\"Steps_per_epoch: {}\".format(steps_per_epoch))\n",
    "mylog(\"Total_steps:{}\".format(total_steps))\n",
    "mylog(\"Steps_per_checkpoint: {}\".format(steps_per_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement = False)\n",
    "config.gpu_options.allow_growth = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    run_options = None\n",
    "    run_metadata = None\n",
    "    mylog_section(\"MODEL/SUMMARY/WRITER\")\n",
    "\n",
    "    mylog(\"Creating Model.. (this can take a few minutes)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22222854428857464"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign([1.0, 1.0, 0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
